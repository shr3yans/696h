# -*- coding: utf-8 -*-
"""hf_finetune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1psPaMZVJcEvfV2vy85a7qCBlpF_hwD1R
"""
'''
!pip install transformers

!pip install datasets
!pip install evaluate
'''
import transformers

from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
from transformers import TrainingArguments
from transformers import AutoModelForSequenceClassification
from transformers import Trainer

splits = ['train[:100]', 'validation[:100]' , 'test[:100]']

raw_datasets = load_dataset("glue", "sst2", split=splits)
#raw_datasets =  load_dataset("glue", "sst2")


checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

print(raw_datasets)

# select train partition only - (idx 0 corresponds to train, 1 to validation and 2 to test)
print("train: {} \n validation: {} \n test: {}".format(raw_datasets[0], raw_datasets[1], raw_datasets[2]))

def tokenize_function(example):
    return tokenizer(example["sentence"], truncation=True)

tokenized_train = raw_datasets[0].map(tokenize_function, batched=True)
tokenized_validation = raw_datasets[1].map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

tokenized_train[0] # print first example in tokenized_train

training_args = TrainingArguments("test-trainer")

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_validation,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()

predictions = trainer.predict(tokenized_validation)
print(predictions.predictions.shape, predictions.label_ids.shape)

import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)



import evaluate

def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "sst2")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=1)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_validation,
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()