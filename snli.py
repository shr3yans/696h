# -*- coding: utf-8 -*-
"""snli.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BlYU9qBdeyGzegv8Gt40L0uu3396MQpL
"""

!pip install transformers

!pip install datasets

import transformers
import torch
#import os
#os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
from transformers import TrainingArguments
from transformers import AutoModelForSequenceClassification
from transformers import Trainer

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

splits = ['train[:100]', 'validation[:100]' , 'test[:100]']

raw_datasets = load_dataset("snli", split=splits)
#raw_datasets =  load_dataset("glue", "sst2")

'''
raw_datasets = raw_datasets["train"][:2]
print(raw_datasets)

train = raw_datasets["train"][:100]
validation = raw_datasets["validation"][:100]
test = raw_datasets["test"][:100]
'''

checkpoint = "roberta-base"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

#print(raw_datasets)

# select train partition only - (idx 0 corresponds to train, 1 to validation and 2 to test)
#print("train: {} \n validation: {} \n test: {}".format(raw_datasets[0], raw_datasets[1], raw_datasets[2]))

train_texts  = []
train_labels = []
test_texts = []
test_labels = []
for i in raw_datasets[0]:
  train_texts.append(str(i["premise"]) + '[ SEP ]' + str(i['hypothesis']))
  train_labels.append(int(i["label"]))
for i in raw_datasets[2]:
  test_texts.append(str(i["premise"]) + '[ SEP ]' + str(i['hypothesis']))
  test_labels.append(int(i["label"]))
#test_texts = raw_datasets[1]["premise"] + [' SEP '] + raw_datasets[1]['hypothesis']

#len(train_texts)

train_encodings = tokenizer(train_texts, truncation=True, padding=True,max_length = 512)
test_encodings = tokenizer(test_texts, truncation=True, padding=True,max_length = 512)

# def tokenize_function(example):
#   return tokenizer(example["premise"], example["hypothesis"], truncation=True)
#
# tokenized_train = raw_datasets[0].map(tokenize_function, batched=True)
# tokenized_validation = raw_datasets[1].map(tokenize_function, batched=True)
# tokenized_test = raw_datasets[2].map(tokenize_function, batched=True)
# #tokenized_dataset = raw_datasets.map(tokenize_function, batched=True)

# train_encodings = tokenizer(train_texts, truncation=True, padding=True,max_length = 512)
#val_encodings = tokenizer(val_texts, truncation=True, padding=True)
# test_encodings = tokenizer(test_texts, truncation=True, padding=True,max_length = 512)


class Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item
    def __len__(self):
        return len(self.labels)
train_set = Dataset(train_encodings, train_labels)
test_set = Dataset(test_encodings, test_labels)


# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

#train_set

#tokenized_train[0] # print first example in tokenized_train

'''
training_args = TrainingArguments("test-trainer")

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_validation,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
'''

#trainer.train()

#predictions = trainer.predict(tokenized_validation)
#print(predictions.predictions.shape, predictions.label_ids.shape)

import numpy as np

#preds = np.argmax(predictions.predictions, axis=-1)

!pip install evaluate

import evaluate

def compute_metrics(eval_preds):
    metric = evaluate.load("snli")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)
model.to(device)
trainer = Trainer(
    model,
    training_args,
    train_dataset=train_set,
    eval_dataset=test_set,
    #data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()
